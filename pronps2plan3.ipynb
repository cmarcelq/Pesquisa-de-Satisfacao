{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cmarc\\AppData\\Local\\Temp\\ipykernel_3964\\722912215.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "C:\\Users\\cmarc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification  # Or BertTokenizer\n",
    "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
    "from transformers import AutoModel \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from torch.nn.functional import softmax\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cmarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cmarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-large-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo pré-treinado e o tokenizador\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', do_lower_case=False)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('neuralmind/bert-large-portuguese-cased')\n",
    "# Carregar stop words em português\n",
    "stop_words_portuguese = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover stopwords em português\n",
    "def remover_stopwords(texto):\n",
    "    palavras = [palavra for palavra in texto.split() if palavra.lower() not in stop_words_portuguese]\n",
    "    return ' '.join(palavras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para realizar a análise de sentimentos\n",
    "def analisar_sentimento(texto):\n",
    "    if not texto or all(t.strip() == '' for t in texto.split()):\n",
    "        # Caso o texto esteja vazio ou seja uma lista de textos vazios, retornar valores padrão\n",
    "        return 'Indefinido', [0.5, 0.5]  # Sentimento indefinido, probabilidade neutra\n",
    "\n",
    "    # Tokenizar o texto e converter para tensores\n",
    "    tokens = tokenizer.encode(texto, return_tensors='pt')\n",
    "\n",
    "    # Passar os tokens pelo modelo\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens)\n",
    "\n",
    "    # Obter probabilidades de cada classe (positiva e negativa)\n",
    "    probabilidade = softmax(outputs.logits, dim=1)\n",
    "    \n",
    "    # Classificar como positivo ou negativo com base na probabilidade\n",
    "    sentimento = \"Positivo\" if probabilidade[0][1] > probabilidade[0][0] else \"Negativo\"\n",
    "\n",
    "    return sentimento, probabilidade.tolist()[0]  # Convertendo para lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair os cinco tópicos mais comuns\n",
    "def extrair_topicos_comuns(comentario):\n",
    "    # Utilizar TF-IDF para vetorização, incluindo stop words em inglês\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, dtype=np.float32)\n",
    "    X = vectorizer.fit_transform(comentario)\n",
    "\n",
    "    # Aplicar a decomposição de matriz não-negativa (NMF)\n",
    "    nmf = NMF(n_components=5, random_state=1)\n",
    "    nmf.fit(X)\n",
    "\n",
    "    # Obter as palavras-chave mais comuns para cada tópico\n",
    "    palavras_chave = []\n",
    "    for i, topic in enumerate(nmf.components_):\n",
    "        top_palavras_indice = topic.argsort()[-5:][::-1]\n",
    "        palavras_chave.append([vectorizer.get_feature_names_out()[indice] for indice in top_palavras_indice])\n",
    "\n",
    "    return palavras_chave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair os cinco tópicos mais comuns para cada agência\n",
    "def extrair_topicos_comuns_por_agencia(planilha):\n",
    "    topicos_agencia = []\n",
    "\n",
    "    for agencia in planilha['agencia'].unique():\n",
    "        # Filtrar a planilha para a agência específica\n",
    "        df_agencia = planilha[planilha['agencia'] == agencia]\n",
    "\n",
    "        # Remover linhas com valores NaN na coluna 'comentario'\n",
    "        df_agencia = df_agencia.dropna(subset=['comentario'])\n",
    "\n",
    "        # Verificar se 'comentario' está presente nas colunas\n",
    "        if 'comentario' in df_agencia.columns:\n",
    "            # Garantir que 'comentario' seja uma string\n",
    "            df_agencia['comentario'] = df_agencia['comentario'].astype(str)\n",
    "\n",
    "            # Remover stopwords da coluna 'comentario'\n",
    "            df_agencia['comentario'] = df_agencia['comentario'].apply(remover_stopwords)\n",
    "            \n",
    "            # Extrair os cinco tópicos mais comuns para a agência\n",
    "            topicos = extrair_topicos_comuns(df_agencia['comentario'])\n",
    "            topicos_agencia.append({'agencia': agencia, 'topicos': topicos})\n",
    "\n",
    "        else:\n",
    "            print(f\"A coluna 'comentario' não foi encontrada para a agência {agencia}.\")\n",
    "\n",
    "    return topicos_agencia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar a planilha\n",
    "planilha_path = 'C:/Users/cmarc/Documents/Faculdade/2023-02/Mineiração/Resultado Pesquisa Satisfação_Atualizada3.xlsx'\n",
    "planilha = pd.read_excel(planilha_path)\n",
    "\n",
    "# Remover linhas com valores NaN na coluna 'comentario'\n",
    "planilha = planilha.dropna(subset=['comentario'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se 'comentario' está presente nas colunas\n",
    "if 'comentario' in planilha.columns:\n",
    "    # Garantir que 'comentario' seja uma string\n",
    "    planilha['comentario'] = planilha['comentario'].astype(str)\n",
    "\n",
    "    # Remover stopwords da coluna 'comentario'\n",
    "    planilha['comentario'] = planilha['comentario'].apply(remover_stopwords)\n",
    "    \n",
    "    # Aplicar a análise de sentimentos à coluna 'comentario'\n",
    "    planilha[['sentimento', 'probabilidade']] = pd.DataFrame(planilha['comentario'].apply(analisar_sentimento).tolist(), index=planilha.index)\n",
    "\n",
    "    # Extrair os tópicos comuns para cada categoria única na coluna 'agencia'\n",
    "    topicos_agencia = extrair_topicos_comuns_por_agencia(planilha)\n",
    "\n",
    "    # Criar uma nova planilha com os tópicos por agência\n",
    "    df_topicos_agencia = pd.DataFrame(topicos_agencia)\n",
    "    df_resultado = pd.concat([planilha[['agencia', 'comentario', 'sentimento', 'probabilidade']]], axis=1)\n",
    "\n",
    "    # Salvar a nova planilha com sentimentos\n",
    "    df_resultado.to_excel('planilha_com_sentimentos_e_topicos_bert.xlsx', index=False)\n",
    "\n",
    "    # Salvar os tópicos comuns em uma planilha separada\n",
    "    df_topicos_agencia.to_excel('planilha_com_topicos_agencia.xlsx', index=False)\n",
    "\n",
    "else:\n",
    "    print(\"A coluna 'comentario' não foi encontrada na planilha.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
